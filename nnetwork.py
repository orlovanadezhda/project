# -*- coding: utf-8 -*-
"""nnetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gVm7eTXK6KYlxfaCcfNXHy9qS72pZZIr
"""

import tensorflow as tf ##импортируем tenserflow для работы с данными
from tensorflow.keras.datasets import mnist #встроенный пакет данных
from tensorflow.keras.utils import to_categorical # функция перевода чисел в матричный вид
## можно переписать как np.matrix([1 if num == i else 0 for num in y])


# импортируем пакет mnist
(x_train,y_train), (x_test,y_test) = mnist.load_data()

x_train = x_train/255 ##нормировка значений, так как содержит информацию о цвете, теперь значения находятся в диапозоне от 0 до 1
x_test = x_test/255

x_train = tf.reshape(tf.cast(x_train, tf.float32),[-1, 28*28])##изменяем вид массива в двумерный
x_test = tf.reshape(tf.cast(x_test, tf.float32),[-1, 28*28])## изменяем тип данных, если считанное изображение имеет тип int8, обычно формат данных изображения должен быть преобразован в float32 перед обучением.

#Преобразование цифр в вид массива
y_train = to_categorical(y_train, 10)

#объявляем класс Dense слоя c выходом размера 128, relu активатор
class DenseNN(tf.Module):
  ##конструктор класса DenseNN по умолчанию активатор relu
  def __init__(self,outputs, activate ="relu"):
    super().__init__()
    self.outputs = outputs## локальные свойства
    self.activate = activate
    self.fl_init = False ## бинарное значение, нужно чтобы в функции call формировать начальные веса ровно один раз

  #выполняется когда объект вызывается как функция
  def __call__(self, x):
    if not self.fl_init:
      ##формируем начальные весовые коэфициенты
      #тут определяем веса - с встроенной функцией. Значения w берутся из нормального распределения с заданным средним значением и стандартным отклонением
        self.w = tf.random.truncated_normal((x.shape[-1], self.outputs), stddev = 0.1, name = "w")
        self.b = tf.zeros([self.outputs],dtype = tf.float32, name ="b") # на месте b массив из нулей
        self.w = tf.Variable(self.w)##Variable задает начальное значение, которое определяет тип и форму переменной. 
        self.b = tf.Variable(self.b)
        self.fl_init = True
    y = x @ self.w + self.b ## полученное значение y равно матричному перемножению исходных данных на веса и добавляется параметр b
    #делаем активацию в зависимости от того, какую передали в DenseNN
    if self.activate == "relu":
      return tf.nn.relu(y)
    elif self.activate == "softmax":
      return tf.nn.softmax(y)

    return y
#делаем первый слой нейронной сети с выходом 128
layer_1 = DenseNN(128)
#Второй слой нейронной сети с выходом 10 и sofmax'ом
layer_2 = DenseNN(10, activate = "softmax")

##Предсказание осуществляется прохождением через оба слоя
def model_predict(x):
  y = layer_1(x)
  y = layer_2(y)
  return y

#Определяем функцию потерь
cross_entropy = lambda y_true, y_pred: tf.reduce_mean(tf.losses.categorical_crossentropy(y_true,y_pred))

#Определяем алгоритм Adam оптимизации обучения нейронной сети с заданным параметром регуляризации
opt = tf.optimizers.Adam(learning_rate = 0.001)

#т.к. весь датасет не можем пропусттить через нейронную сеть, то делим данные на пакеты-батчи
BATCH_SIZE = 32
#5 эпох, значит датасет 5 раз пройдёт через нейронную сеть
EPOCHS = 5

TOTAL = x_train.shape[0] ##строки массива x_train, то есть количество данных для обучения модели

#определяем dataset для обучения
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
#перемешиваем dataset, разделяя на батчи
train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(BATCH_SIZE)

#идём по эпохам и определяем потери
for n in range(EPOCHS):
  loss = 0 ## начальное значение потерь
  for x_batch, y_batch in train_dataset:
    #Tape использую для записи данных
    with tf.GradientTape() as tape:
      ##функция потерь запишем через кросс энтропию
      f_loss = cross_entropy(y_batch, model_predict(x_batch))
    loss = loss + f_loss
    ##суммируем потери

    ##автоматическое дифференцирование - градиентный спуск
    grads = tape.gradient(f_loss, [layer_1. trainable_variables, layer_2.trainable_variables])
##оптимизация для каждого из слоев
    opt.apply_gradients(zip(grads[0],layer_1.trainable_variables))
    opt.apply_gradients(zip(grads[1],layer_2.trainable_variables))
  print(loss.numpy()) #вывод потерь для каждой эпохи(всего 5)

#вызываем предикт в котором определяем слои
y = model_predict(x_test)
y2 = tf.argmax(y,axis=1).numpy() # y2 возвращает максимум по оси
#считаем точность 
acc = len(y_test[y_test == y2])/y_test.shape[0]*100 #считаем количеств совпадений, деленных на размер датасета
print(acc,"%")