# -*- coding: utf-8 -*-
"""log_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U8qVHshHn3elOCOb0MMPokBlkpOiAQrY
"""

import pandas as pd ##импортируем нужные библиотеки для работы с данными
import numpy as np
from scipy import optimize ##нужна для минимизация функции, то есть использует алгоритм нелинейного сопряженного градиента
import scipy.io ##поддерживает чтение файлов .mat

## Описание всех функций, которые потребуюся для решения задачи
def sigmoid(z):
	f = 1.0 / (1.0 + np.exp(-z)) ##основная функция логистической регрессии - сигмоид
	return f

##Функция перемножающая веса с матрицей Х, по сути здесь идет подстановка весов в уравнение - сигмоид, обычно ее называют функцией гипотезы.
def funcMultiply(X, weights):
	[a, b] = np.shape(X) ##Передаем в a,b размеры массива  b = 28*28 = 784 пиксели, размер a зависит от количества элементов для обучения модели
	weights = weights.reshape(1, b) ## для умножения меняем размер - получается строка, которая при транспонировании становится столбцом, что нужно для умножения
	z = X @ (weights.T) ##Матричное умножение массива X на транспонированный массив - получаем результат для каждого элемента тренировочных данных(для каждой цифры)
	res = sigmoid(z)
	return res

##Функция затрат - это мера того, насколько ошибочна модель с точки зрения ее способности оценивать взаимосвязь между X и y.
##Она возвращает J(weights) - среднее значение ошибки всех выборок
def func_cost(weights, X, y, lmbda):
  J = 0 ## Первоначально равна нулю
  [a, b] = np.shape(X) ##Передаем в a,b размеры массива
  weights = weights.reshape(1, b)
  #меняем размер для вектора весов - получается строка
  #считаем функцию потерь по формуле: -(массив y умножается на логарифм(sigmoid(z)) + (1 - y)умножается на логарифм(1 - sigmoid(z))) и делется на a - количество элементов в тренировочной выборке
  func_lloss = (-y.T.dot(np.log(funcMultiply(X, weights))) - (1 - y).T.dot(np.log(1 - funcMultiply(X, weights)))) / a
  reg_element = (lmbda / (2 * a)) * (np.sum(weights.dot(weights.T)))
  J = func_lloss + reg_element ##считаем по формуле среднее значение ошибки 
  ##J = логистическая функция потерь, которую мы просуммировали по всей выборке + коэфициент регуляризации, деленный на количество слагаемых, умноженный на скалярное произведение весов
  return J

#Градиентный спуск - метод нахождения локального минимума или максимума функции с помощью движения вдоль градиента.
def gradientDescent(weights, X, y, lmbda):
  [a, b] = np.shape(X) ##Передаем в a,b размеры массива
  weights = weights.reshape(1, b) ##меняем размер для вектора весов - получается строка
  grad = np.zeros((1, b)) ##создаем с помощью numpy массив-строку размера b, она заполнена нулями по умолчанию
##Вычисляем градиент по формуле - берем соседние значение и скалярно умножаем на матрицу ошибок + регуляризация
  for i in range(0, b):
    grad[:,i:] = ((funcMultiply(X, weights) - y).T.dot(X[:,i:i+1])/a) + ((lmbda/a) * weights[:,i:i+1])
  grad = grad.T## транспонируем массив градиентов
  grad = grad.ravel() ##возвращает сжатый до одной оси массив
  return grad

#Классификация с помощью встроенное функции оптимизации
def opt_class(X, y, weights, lmbda, num_counts):
  [a, b] = np.shape(X) ##Передаем в a,b размеры массива
  weights = weights.reshape(1, b)
  result = optimize.fmin_cg(func_cost, fprime = gradientDescent, x0 = weights, args = (X, y, lmbda), maxiter = 50, disp=False, full_output = True)
  #func_cost должна быть сведена к минимуму,  x0 - одномерный массив весов, который регулируется функцией,fprime - функция, которая возвращает градиент
  #args - аргументы для двух функций,  maxiter - максимальное количество итераций
  return result

#Функция для прогнозирования значений
def predict(right_weights, X, y): ##Передаются конечные веса
  a = np.shape(X)[0] #количество элементов в наборе цифр
  results = np.zeros((a, 1))## столбец с нулями размера a
  res_max = sigmoid(X.dot(right_weights.T))## результаты работы сигмоиды с конечными весами
  for i in range(0, a):#нахождение предсказанных элементов
    results[i] = np.argmax(res_max[i, :])#возвращает максимальное значение среди столбцов одной строки
    
		
	# Вычисление точности
  correct = 0
  for i in range(a):
    if(results[i] == y[i]):
      correct = correct + 1
  accuracy = (correct / a) * 100  #точность вычисляется делением всех верных предсказаний на общее число элементов
  return accuracy

# Обучение
def train(X, y, lmbda, num_counts):
	[a, b] = np.shape(X) ##Передаем в a,b размеры массива
	right_weights = np.zeros((num_counts, b)) ##инициализируем массив нулями
	for i in range(num_counts):## нахожение весов для каждого числа
		y_bool = np.matrix([1 if num == i else 0 for num in y])##переведем значение числа в матричный вид пример:0000100000
		y_bool = y_bool.T
		right_weights[i,:] = opt_class(X, y_bool, np.zeros((1, b)), lmbda, num_counts)[0] ##оптимизация - для каждой цифры подбираются наиболее подходящие значения весов
	return right_weights


##Чтение информации из файла
mat = scipy.io.loadmat('mnist.mat')
X = mat['X']
y = mat['y']

num_counts = 10 ##Назначаем значение 10, так как всего 10 цифр от 0 до 9
##Параметр регулязации
reg_param = 0.01 

train_model = train(X, y, reg_param, num_counts)##обучаем модель
accuracy = predict(train_model, X, y)##предсказание и вычисление точности
print(accuracy,"%")